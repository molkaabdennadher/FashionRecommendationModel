import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import TomekLinks
import warnings

warnings.filterwarnings('ignore')

# ============================================================
# CONFIGURATION
# ============================================================

INPUT_FILE = r"C:\dataware\venv\finaledata_enrichi.xlsx"
OUTPUT_FILE = r"C:\dataware\venv\data_prepared_for_ml.xlsx"

print("=" * 80)
print("PIPELINE DE PR√âPARATION DES DONN√âES POUR ML")
print("Mod√®les cibl√©s: R√©seaux de Neurones & XGBoost")
print("=" * 80)

# ============================================================
# √âTAPE 1: CHARGEMENT DES DONN√âES
# ============================================================

print("\n[√âTAPE 1/10] Chargement des donn√©es...")

df = pd.read_excel(INPUT_FILE)
print(f"‚úì Donn√©es charg√©es: {len(df):,} lignes, {df.shape[1]} colonnes")
print(f"  M√©moire utilis√©e: {df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB")

# Sauvegarde des donn√©es originales pour comparaison
df_original = df.copy()
# ============================================================
# √âTAPE 2: SUPPRESSION DES DOUBLONS
# ============================================================

print("\n[√âTAPE 2/10] Suppression des doublons...")

nb_avant = len(df)
df = df.drop_duplicates()
nb_apres = len(df)
nb_doublons = nb_avant - nb_apres

print(f"‚úì Doublons supprim√©s: {nb_doublons:,}")
print(f"  Lignes restantes: {nb_apres:,} ({nb_apres / nb_avant * 100:.2f}% des donn√©es)")

# ============================================================
# √âTAPE 2b: SUPPRESSION DES COLONNES NON SIGNIFICATIVES
# ============================================================

print("\n[√âTAPE 2b/10] Suppression des colonnes non significatives...")

# Liste des colonnes √† supprimer (date et autres colonnes non significatives)
colonnes_a_supprimer = []

# Identifier les colonnes de date
colonnes_date = []
for col in df.columns:
    if any(mot in col.lower() for mot in ['date', 'time', 'jour', 'month', 'year', 'timestamp']):
        colonnes_date.append(col)

if colonnes_date:
    print(f"  Colonnes de date identifi√©es: {colonnes_date}")
    colonnes_a_supprimer.extend(colonnes_date)

# Autres colonnes non significatives (ajuster selon votre cas)
colonnes_non_significatives = ['id', 'index', 'unnamed', 'user_id']  # Exemples
for col in df.columns:
    if any(mot in col.lower() for mot in colonnes_non_significatives) and col not in colonnes_a_supprimer:
        colonnes_a_supprimer.append(col)

# Supprimer les colonnes (si elles existent)
colonnes_supprimees = []
for col in colonnes_a_supprimer:
    if col in df.columns:
        df = df.drop(columns=[col])
        colonnes_supprimees.append(col)

if colonnes_supprimees:
    print(f"‚úì Colonnes supprim√©es: {colonnes_supprimees}")
    print(f"  Colonnes restantes: {len(df.columns)}")
else:
    print("‚úì Aucune colonne non significative √† supprimer")
# ============================================================
# √âTAPE 3: CR√âATION DE LA VARIABLE CIBLE BINAIRE (rating_binary)
# ============================================================

print("\n[√âTAPE 3/10] Cr√©ation de rating_binary (Positif/N√©gatif)...")

# V√©rifier quelle colonne de rating existe
rating_col = None
for col in ['rating', 'Rating', 'review_rating', 'score']:
    if col in df.columns:
        rating_col = col
        break

if rating_col is None:
    print("‚ö† Aucune colonne de rating trouv√©e. Colonnes disponibles:")
    print(df.columns.tolist())
    raise ValueError("Colonne de rating non trouv√©e!")

print(f"  Colonne de rating utilis√©e: '{rating_col}'")

# Analyse de la distribution des ratings
print(f"\n  Distribution des ratings originaux:")
print(df[rating_col].value_counts().sort_index())
print(f"\n  Statistiques: min={df[rating_col].min()}, max={df[rating_col].max()}, "
      f"moyenne={df[rating_col].mean():.2f}, m√©diane={df[rating_col].median()}")

# UTILISER UN SEUIL FIXE DE 6
SEUIL_FIXE = 6
print(f"\n  SEUIL FIXE APPLIQU√â: {SEUIL_FIXE}")

# Cr√©er rating_binary avec >= 6 pour Positif
df['rating_binary'] = df[rating_col].apply(
    lambda x: 'Positif' if x >= SEUIL_FIXE else 'N√©gatif'
)

print(f"\n  R√®gle appliqu√©e: rating ‚â• {SEUIL_FIXE} = Positif, rating < {SEUIL_FIXE} = N√©gatif")
print(f"\n  Distribution de rating_binary:")
distribution = df['rating_binary'].value_counts()
print(distribution)

if 'Positif' in distribution.index and 'N√©gatif' in distribution.index:
    ratio = distribution['Positif'] / distribution['N√©gatif']
    print(f"\n  Ratio Positif/N√©gatif: {ratio:.2f}")
    if ratio < 0.2 or ratio > 5:
        print(f"  ‚ö†Ô∏è  D√©s√©quilibre significatif d√©tect√© - TOMEK sera appliqu√©")
else:
    print("\n  ‚ùå ERREUR: Une seule classe d√©tect√©e!")
    print(f"  ‚Üí Le seuil {SEUIL_FIXE} ne permet pas de s√©parer les donn√©es")

    # Essayer avec un seuil plus bas si n√©cessaire
    SEUIL_FIXE = 5
    print(f"  ‚Üí Essai avec nouveau seuil: {SEUIL_FIXE}")

    df['rating_binary'] = df[rating_col].apply(
        lambda x: 'Positif' if x >= SEUIL_FIXE else 'N√©gatif'
    )

    distribution = df['rating_binary'].value_counts()
    print(f"\n  Nouvelle distribution de rating_binary:")
    print(distribution)

    if 'Positif' in distribution.index and 'N√©gatif' in distribution.index:
        ratio = distribution['Positif'] / distribution['N√©gatif']
        print(f"\n  Ratio Positif/N√©gatif: {ratio:.2f}")
    else:
        raise ValueError("Impossible de cr√©er deux classes avec cette distribution de ratings!")

# ============================================================
# √âTAPE 4: GESTION DES VALEURS MANQUANTES
# ============================================================

print("\n[√âTAPE 4/10] Gestion des valeurs manquantes...")

# Afficher les colonnes avec des valeurs manquantes
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing.index,
    'Valeurs_manquantes': missing.values,
    'Pourcentage': missing_pct.values
})
missing_df = missing_df[missing_df['Valeurs_manquantes'] > 0].sort_values(
    'Valeurs_manquantes', ascending=False
)

if len(missing_df) > 0:
    print(f"\n  Colonnes avec valeurs manquantes:")
    print(missing_df.to_string(index=False))

    # Strat√©gie de gestion des valeurs manquantes
    for col in df.columns:
        if df[col].isnull().sum() > 0:
            if df[col].dtype in ['int64', 'float64']:
                # Variables num√©riques: remplir avec la m√©diane
                df[col].fillna(df[col].median(), inplace=True)
            else:
                # Variables cat√©gorielles: remplir avec le mode ou 'Unknown'
                if df[col].mode().empty:
                    df[col].fillna('Unknown', inplace=True)
                else:
                    df[col].fillna(df[col].mode()[0], inplace=True)

    print(f"\n‚úì Valeurs manquantes trait√©es")
else:
    print("‚úì Aucune valeur manquante d√©tect√©e")

# ============================================================
# √âTAPE 5: IDENTIFICATION DES TYPES DE VARIABLES
# ============================================================

print("\n[√âTAPE 5/10] Identification des types de variables...")

# S√©parer les colonnes num√©riques et cat√©gorielles
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Exclure la variable cible et l'ID
if 'rating_binary' in categorical_cols:
    categorical_cols.remove('rating_binary')
if 'user_id' in categorical_cols:
    categorical_cols.remove('user_id')
if 'user_id' in numeric_cols:
    numeric_cols.remove('user_id')
if rating_col in numeric_cols:
    numeric_cols.remove(rating_col)

print(f"\n  Variables num√©riques ({len(numeric_cols)}):")
for col in numeric_cols:
    print(f"    - {col}")

print(f"\n  Variables cat√©gorielles ({len(categorical_cols)}):")
for col in categorical_cols:
    print(f"    - {col}")

# ============================================================
# √âTAPE 6: ENCODAGE DES VARIABLES CAT√âGORIELLES (Label Encoding)
# ============================================================

print("\n[√âTAPE 6/10] Encodage des variables cat√©gorielles (Label Encoding)...")

# Dictionnaire pour stocker les encodeurs (utile pour l'inf√©rence future)
label_encoders = {}

for col in categorical_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col + '_encoded'] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

        print(f"  ‚úì {col}: {len(le.classes_)} cat√©gories uniques ‚Üí {col}_encoded")

# Encoder √©galement la variable cible
le_target = LabelEncoder()
df['rating_binary_encoded'] = le_target.fit_transform(df['rating_binary'])
label_encoders['rating_binary'] = le_target

print(f"\n‚úì {len(categorical_cols)} variables cat√©gorielles encod√©es")
print(f"  Classes de rating_binary: {le_target.classes_}")

# ============================================================
# √âTAPE 7: NORMALISATION DES VARIABLES NUM√âRIQUES
# ============================================================

print("\n[√âTAPE 7/10] Normalisation des variables num√©riques (StandardScaler)...")

scaler = StandardScaler()

# Cr√©er des colonnes normalis√©es
numeric_cols_to_scale = [col for col in numeric_cols if col in df.columns]

if numeric_cols_to_scale:
    df_scaled = pd.DataFrame(
        scaler.fit_transform(df[numeric_cols_to_scale]),
        columns=[col + '_scaled' for col in numeric_cols_to_scale],
        index=df.index
    )

    df = pd.concat([df, df_scaled], axis=1)

    print(f"‚úì {len(numeric_cols_to_scale)} variables num√©riques normalis√©es")
    print(f"  Moyenne ‚âà 0, √âcart-type ‚âà 1")
else:
    print("‚ö† Aucune variable num√©rique √† normaliser")

# ============================================================
# √âTAPE 8: MATRICE DE CORR√âLATION
# ============================================================

print("\n[√âTAPE 8/10] Analyse de corr√©lation...")

# Cr√©er un DataFrame avec toutes les variables encod√©es et normalis√©es
df_for_corr = pd.DataFrame()

# Ajouter les variables num√©riques normalis√©es
for col in numeric_cols_to_scale:
    if col + '_scaled' in df.columns:
        df_for_corr[col] = df[col + '_scaled']

# Ajouter les variables encod√©es
for col in categorical_cols:
    if col + '_encoded' in df.columns:
        df_for_corr[col + '_enc'] = df[col + '_encoded']

# Ajouter la variable cible encod√©e
df_for_corr['rating_binary'] = df['rating_binary_encoded']

# Calculer la matrice de corr√©lation
correlation_matrix = df_for_corr.corr()

print(f"‚úì Matrice de corr√©lation calcul√©e ({correlation_matrix.shape[0]}x{correlation_matrix.shape[1]})")

# Corr√©lations avec la variable cible
target_correlations = correlation_matrix['rating_binary'].drop('rating_binary').sort_values(
    ascending=False)

print(f"\nüìä Top 10 corr√©lations POSITIVES avec rating_binary:")
print(target_correlations.head(10))

print(f"\nüìä Top 10 corr√©lations N√âGATIVES avec rating_binary:")
print(target_correlations.tail(10))

# Identifier les features fortement corr√©l√©es entre elles (multicolin√©arit√©)
print(f"\n‚ö†Ô∏è  Paires de features fortement corr√©l√©es (|r| > 0.8):")
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.8:
            high_corr_pairs.append({
                'Feature_1': correlation_matrix.columns[i],
                'Feature_2': correlation_matrix.columns[j],
                'Correlation': correlation_matrix.iloc[i, j]
            })

if high_corr_pairs:
    df_high_corr = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)
    print(df_high_corr.to_string(index=False))
else:
    print("  Aucune corr√©lation forte d√©tect√©e (bon signe!)")

# Sauvegarder la matrice de corr√©lation
correlation_file = r"C:\dataware\venv\correlation_matrix.xlsx"
with pd.ExcelWriter(correlation_file, engine='openpyxl') as writer:
    correlation_matrix.to_excel(writer, sheet_name='Matrice_Correlation')
    target_correlations.to_frame('Correlation').to_excel(writer, sheet_name='Corr_avec_Target')
    if high_corr_pairs:
        df_high_corr.to_excel(writer, sheet_name='Multicolinearite', index=False)

print(f"\n‚úì Matrice de corr√©lation export√©e: {correlation_file}")

# Visualisation optionnelle avec matplotlib/seaborn (si disponible)
try:
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Figure 1: Heatmap de corr√©lation compl√®te
    plt.figure(figsize=(16, 14))
    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('Matrice de Corr√©lation Compl√®te', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(r"C:\dataware\venv\correlation_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()

    # Figure 2: Corr√©lations avec la target
    plt.figure(figsize=(10, 8))
    top_features = pd.concat([target_correlations.head(15), target_correlations.tail(15)]).sort_values()
    colors = ['red' if x < 0 else 'green' for x in top_features.values]
    plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)
    plt.yticks(range(len(top_features)), top_features.index, fontsize=9)
    plt.xlabel('Corr√©lation avec rating_binary', fontsize=12)
    plt.title('Top 30 Features corr√©l√©es avec rating_binary', fontsize=14, fontweight='bold')
    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig(r"C:\dataware\venv\correlation_target.png", dpi=300, bbox_inches='tight')
    plt.close()

    print(f"\n‚úì Graphiques de corr√©lation sauvegard√©s:")
    print(f"  - correlation_heatmap.png")
    print(f"  - correlation_target.png")

except ImportError:
    print("\n‚ö†Ô∏è  matplotlib/seaborn non disponible - graphiques non g√©n√©r√©s")
    print("   Installez avec: pip install matplotlib seaborn")

# ============================================================
# √âTAPE 9: PR√âPARATION POUR TOMEK LINKS (√©quilibrage)
# ============================================================

print("\n[√âTAPE 9/10] Application de TOMEK Links pour √©quilibrage...")

# Pr√©parer les features et la target
# Utiliser les colonnes encod√©es et normalis√©es
feature_cols = ([col + '_encoded' for col in categorical_cols if col + '_encoded' in df.columns] +
                [col + '_scaled' for col in numeric_cols_to_scale if col + '_scaled' in df.columns])

print(f"\n  Features s√©lectionn√©es pour le mod√®le ({len(feature_cols)}):")
for i, feat in enumerate(feature_cols, 1):
    print(f"    {i}. {feat}")

X = df[feature_cols].values
y = df['rating_binary_encoded'].values

print(f"\n  Distribution avant TOMEK:")
unique, counts = np.unique(y, return_counts=True)
for val, count in zip(unique, counts):
    label = le_target.inverse_transform([val])[0]
    print(f"    {label}: {count:,} ({count / len(y) * 100:.2f}%)")

# Appliquer TOMEK Links
tomek = TomekLinks(sampling_strategy='auto')
X_tomek, y_tomek = tomek.fit_resample(X, y)

print(f"\n  Distribution apr√®s TOMEK:")
unique, counts = np.unique(y_tomek, return_counts=True)
for val, count in zip(unique, counts):
    label = le_target.inverse_transform([val])[0]
    print(f"    {label}: {count:,} ({count / len(y_tomek) * 100:.2f}%)")

print(f"\n‚úì √âchantillons supprim√©s par TOMEK: {len(X) - len(X_tomek):,}")

# Cr√©er un DataFrame avec les donn√©es √©quilibr√©es
df_balanced = pd.DataFrame(X_tomek, columns=feature_cols)
df_balanced['rating_binary_encoded'] = y_tomek
df_balanced['rating_binary'] = le_target.inverse_transform(y_tomek)

# ============================================================
# √âTAPE 10: SPLIT TRAIN/TEST
# ============================================================

print("\n[√âTAPE 10/10] Split Train/Test (80/20)...")

X_train, X_test, y_train, y_test = train_test_split(
    X_tomek, y_tomek,
    test_size=0.2,
    random_state=42,
    stratify=y_tomek
)

print(f"‚úì Train set: {len(X_train):,} √©chantillons")
print(f"‚úì Test set:  {len(X_test):,} √©chantillons")

# Distribution dans les sets
print(f"\n  Distribution Train:")
unique, counts = np.unique(y_train, return_counts=True)
for val, count in zip(unique, counts):
    label = le_target.inverse_transform([val])[0]
    print(f"    {label}: {count:,} ({count / len(y_train) * 100:.2f}%)")

print(f"\n  Distribution Test:")
unique, counts = np.unique(y_test, return_counts=True)
for val, count in zip(unique, counts):
    label = le_target.inverse_transform([val])[0]
    print(f"    {label}: {count:,} ({count / len(y_test) * 100:.2f}%)")

# ============================================================
# EXPORTATION DES DONN√âES PR√âPAR√âES
# ============================================================

print("\n" + "=" * 80)
print("EXPORTATION DES DONN√âES")
print("=" * 80)

# Cr√©er les DataFrames pour l'export
df_train = pd.DataFrame(X_train, columns=feature_cols)
df_train['rating_binary_encoded'] = y_train
df_train['rating_binary'] = le_target.inverse_transform(y_train)

df_test = pd.DataFrame(X_test, columns=feature_cols)
df_test['rating_binary_encoded'] = y_test
df_test['rating_binary'] = le_target.inverse_transform(y_test)

# Export vers Excel avec plusieurs onglets
with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
    # Onglet 1: Donn√©es compl√®tes √©quilibr√©es
    df_balanced.to_excel(writer, sheet_name='Data_Balanced', index=False)

    # Onglet 2: Train set
    df_train.to_excel(writer, sheet_name='Train_Set', index=False)

    # Onglet 3: Test set
    df_test.to_excel(writer, sheet_name='Test_Set', index=False)

    # Onglet 4: Mapping des encodeurs
    encoding_info = []
    for col, encoder in label_encoders.items():
        for idx, classe in enumerate(encoder.classes_):
            encoding_info.append({
                'Colonne': col,
                'Valeur_originale': classe,
                'Valeur_encod√©e': idx
            })

    df_encoding = pd.DataFrame(encoding_info)
    df_encoding.to_excel(writer, sheet_name='Encodage_Mapping', index=False)

    # Onglet 5: Liste des features
    features_info = pd.DataFrame({
        'Feature': feature_cols,
        'Type': ['Cat√©gorielle encod√©e' if '_encoded' in f else 'Num√©rique normalis√©e'
                 for f in feature_cols]
    })
    features_info.to_excel(writer, sheet_name='Features_Liste', index=False)

    # Onglet 6: Rapport de pr√©paration
    rapport = pd.DataFrame({
        '√âtape': [
        'Donn√©es originales',
        'Apr√®s suppression doublons',
        'Apr√®s TOMEK Links',
        'Train set (80%)',
        'Test set (20%)',
        'Nombre de features',
        'Variables cat√©gorielles encod√©es',
        'Variables num√©riques normalis√©es',
        'Seuil rating_binary',
        'Classe positive',
        'Classe n√©gative'
    ],
    'Valeur': [
        f"{len(df_original):,}",
        f"{len(df):,}",
        f"{len(df_balanced):,}",
        f"{len(df_train):,}",
        f"{len(df_test):,}",
        len(feature_cols),
        len(categorical_cols),
        len(numeric_cols_to_scale),
        f">={SEUIL_FIXE}",  # ‚Üê CORRECTION ICI
        le_target.classes_[1] if len(le_target.classes_) > 1 else 'N/A',
        le_target.classes_[0]
    ]
})
    rapport.to_excel(writer, sheet_name='Rapport', index=False)

    # Onglet 7: Matrice de corr√©lation
    correlation_matrix.to_excel(writer, sheet_name='Correlation_Matrix')

    # Onglet 8: Corr√©lations avec target
    target_correlations.to_frame('Correlation').to_excel(writer, sheet_name='Corr_Target')

print(f"\n‚úì Fichier Excel cr√©√©: {OUTPUT_FILE}")
print(f"  Contient 8 onglets:")
print(f"    1. Data_Balanced       - Donn√©es compl√®tes √©quilibr√©es")
print(f"    2. Train_Set           - Ensemble d'entra√Ænement (80%)")
print(f"    3. Test_Set            - Ensemble de test (20%)")
print(f"    4. Encodage_Mapping    - Correspondance valeurs encod√©es")
print(f"    5. Features_Liste      - Liste des features pour les mod√®les")
print(f"    6. Rapport             - R√©sum√© de la pr√©paration")
print(f"    7. Correlation_Matrix  - Matrice de corr√©lation compl√®te")
print(f"    8. Corr_Target         - Corr√©lations avec rating_binary")

# Sauvegarder √©galement les objets Python pour utilisation directe
import pickle

objects_to_save = {
    'X_train': X_train,
    'X_test': X_test,
    'y_train': y_train,
    'y_test': y_test,
    'feature_cols': feature_cols,
    'label_encoders': label_encoders,
    'scaler': scaler,
    'le_target': le_target
}

pickle_file = r"C:\dataware\venv\ml_objects.pkl"
with open(pickle_file, 'wb') as f:
    pickle.dump(objects_to_save, f)

print(f"\n‚úì Objets Python sauvegard√©s: {pickle_file}")

# ============================================================
# R√âSUM√â FINAL
# ============================================================

print("\n" + "=" * 80)
print("R√âSUM√â DE LA PR√âPARATION")
print("=" * 80)
print(f"üìä Donn√©es originales:              {len(df_original):,} lignes")
print(f"üìä Apr√®s nettoyage:                 {len(df):,} lignes")
print(f"üìä Apr√®s √©quilibrage (TOMEK):       {len(df_balanced):,} lignes")
print(f"üìä Train set:                       {len(df_train):,} lignes")
print(f"üìä Test set:                        {len(df_test):,} lignes")
print(f"\nüîß Features pr√©par√©es:              {len(feature_cols)}")
print(f"   - Cat√©gorielles encod√©es:        {len(categorical_cols)}")
print(f"   - Num√©riques normalis√©es:        {len(numeric_cols_to_scale)}")
print(f"\nüéØ Variable cible:                  rating_binary")
print(f"   - Classes: {le_target.classes_}")
print(f"   - Seuil utilis√©: >={SEUIL_FIXE}")
print(f"\n‚úÖ Donn√©es pr√™tes pour:")
print(f"   - R√©seaux de Neurones (features normalis√©es)")
print(f"   - XGBoost (features encod√©es)")
print("=" * 80)

print("\nüöÄ Donn√©es pr√™tes pour l'entra√Ænement des mod√®les!")
print("\nüí° Prochaines √©tapes:")
print("   1. Charger les donn√©es depuis 'Train_Set' et 'Test_Set'")
print("   2. Entra√Æner vos mod√®les (NN, XGBoost)")
print("   3. √âvaluer les performances sur Test_Set")
print("   4. Utiliser les encodeurs sauvegard√©s pour de nouvelles pr√©dictions")